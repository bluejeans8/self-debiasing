# On Bias Mitigation and Size of Language Models

#### Team 7 (Jeongyeon Seo, Jinseok Kim, Neul Oh)

Large language models (LLMs) have proven to
possess great potential in benefiting the society,
while questions and concerns remain in their possible negative impact. We investigate the relationship between language model sizes and one
such concern: bias. We focus on identity attack,
profanity, sexual explicitness, threat, toxicity and
severe toxicity, using the Perspective API. Our
study confirms that the ability to Self-Diagnose
and Self-Debias increases with model size (e.g.,
comparing GPT-2 series and LLaMA), and that
instruction-based fine-tuning can mitigate bias in
language models by increasing human alignment
(e.g., comparing LLaMA and Alpaca). We conclude with discussions on how to cope with bias
and toxicity increasing with language model size,
for example by instruction-based fine-tuning. For more details, please refer to our final project paper ["On Bias Mitigation and Size of Language Models"](https://github.com/bluejeans8/self-debiasing/files/12064363/AI_ethics_Team_7_Final.pdf)
