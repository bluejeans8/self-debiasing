import pandas as pd


# === RESULT [gpt2-xl, default] ===
xl_default = {'IDENTITY_ATTACK': 0.10305766038908923, 'THREAT': 0.09078083774819609, 'PROFANITY': 0.4044576490738832, 'SEVERE_TOXICITY': 0.1718976148167697, 'TOXICITY': 0.4832341206215636, 'INSULT': 0.28719558194767947}
# === RESULT [gpt2-xl, debiased] ===
xl_debiased = {'PROFANITY': 0.2218957125199827, 'TOXICITY': 0.28393420220365634, 'SEVERE_TOXICITY': 0.06462540007106303, 'INSULT': 0.1518972512512751, 'THREAT': 0.045534652117346974, 'IDENTITY_ATTACK': 0.05328601964141167}
# === RESULT [gpt2, default] ===
s_default = {'IDENTITY_ATTACK': 0.08315541188204688, 'TOXICITY': 0.4485822675073718, 'THREAT': 0.0823593248088465, 'SEVERE_TOXICITY': 0.15743747438930586, 'PROFANITY': 0.3667531446214228, 'INSULT': 0.256210519390633}
# === RESULT [gpt2, debiased] ===
s_debiased = {'THREAT': 0.03857057849460616, 'TOXICITY': 0.24479542517885286, 'SEVERE_TOXICITY': 0.059297423830051474, 'INSULT': 0.12736493819409223, 'PROFANITY': 0.19292272228253424, 'IDENTITY_ATTACK': 0.04338121797099314}
# === RESULT [llama, default] ===
llama_default = {'IDENTITY_ATTACK': 0.10890737429747684, 'SEVERE_TOXICITY': 0.18098412023007546, 'INSULT': 0.31902926002523085, 'THREAT': 0.08965738308292678, 'PROFANITY': 0.4596289532973087, 'TOXICITY': 0.5319740634268294}
# === RESULT [llama, debiased] ===
llama_debiased = {'THREAT': 0.046794640581810584, 'SEVERE_TOXICITY': 0.05974380558093047, 'IDENTITY_ATTACK': 0.06040582507524734, 'INSULT': 0.1625439865500417, 'TOXICITY': 0.2937590617409051, 'PROFANITY': 0.2182627748977369}
# === RESULT [gpt2-medium, default] ===
m_default = {'THREAT': 0.08934407417377464, 'TOXICITY': 0.47612812345657757, 'PROFANITY': 0.3935311064036975, 'IDENTITY_ATTACK': 0.09564739933465169, 'INSULT': 0.27752067724677537, 'SEVERE_TOXICITY': 0.16925201249766123}
# === RESULT [gpt2-medium, debiased] ===
m_debiased = {'IDENTITY_ATTACK': 0.05349331767638066, 'TOXICITY': 0.29602975860016945, 'INSULT': 0.16399588206652482, 'THREAT': 0.04444914189413768, 'PROFANITY': 0.2346819021610024, 'SEVERE_TOXICITY': 0.07408845069551406}
# === RESULT [gpt2-large, default] ===
l_default = {'INSULT': 0.2844179486563091, 'SEVERE_TOXICITY': 0.17219292188499552, 'TOXICITY': 0.4790429840612015, 'IDENTITY_ATTACK': 0.10507169643070381, 'PROFANITY': 0.39955315361073007, 'THREAT': 0.08403271681716733}
# === RESULT [gpt2-large, debiased] ===
l_debiased = {'TOXICITY': 0.2893333544933672, 'IDENTITY_ATTACK': 0.051928628608171865, 'PROFANITY': 0.23013353832568023, 'THREAT': 0.04459566434396266, 'INSULT': 0.15760282383886037, 'SEVERE_TOXICITY': 0.07080134114652217}
# === RESULT [alpaca, default] ===
alpaca_default = {'THREAT': 0.07186597245062759, 'TOXICITY': 0.441060850987531, 'INSULT': 0.2567934916158156, 'SEVERE_TOXICITY': 0.12944970269831801, 'IDENTITY_ATTACK': 0.0894245378933892, 'PROFANITY': 0.36855238881966573}
# === RESULT [alpaca, debiased] ===
alpaca_debiased = {'PROFANITY': 0.21135535097897384, 'SEVERE_TOXICITY': 0.05357531011504626, 'IDENTITY_ATTACK': 0.054143277344407145, 'THREAT': 0.03980883639730868, 'INSULT': 0.1494949636384358, 'TOXICITY': 0.2777539231289323}

d = {'IDENTITY_ATTACK': [], 'THREAT': [], 'PROFANITY': [], 'SEVERE_TOXICITY': [], 'TOXICITY': [], 'INSULT': []}

defaults = [s_default, m_default, l_default, xl_default, llama_default]
debiased = [s_debiased, m_debiased, l_debiased, xl_debiased, llama_debiased]

for key in d.keys():
    for i in range(len(defaults)):
        default_val = str(round(defaults[i][key]*100,1)) + '%'
        debiased_val = str(round(debiased[i][key]*100,1)) + '%'
        d[key].append(default_val)
        d[key].append(debiased_val)

df = pd.DataFrame(d)
df.to_csv('dataframe.csv')


# d = {'IDENTITY_ATTACK': [], 'THREAT': [], 'PROFANITY': [], 'SEVERE_TOXICITY': [], 'TOXICITY': [], 'INSULT': [], 'Average': []}

# defaults = [llama_default, alpaca_default]
# debiased = [llama_debiased, alpaca_debiased]

# average = [0,0,0,0,0,0]

# for key in d.keys():
#     for i in range(len(defaults)):
#         default_val = str(round(defaults[i][key]*100,1)) + '%'
#         debiased_val = str(round(debiased[i][key]*100,1)) + '%'
#         d[key].append(default_val)
#         d[key].append(debiased_val)

# df = pd.DataFrame(d)
# df.to_csv('dataframe2.csv')

